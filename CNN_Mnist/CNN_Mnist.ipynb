{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X = np.append(x_train, x_test, axis=0)\n",
    "y = np.append(y_train, y_test, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we appended all the date to be able to shuffle it and split it after that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we seperated the data and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PGViYNM8iZPX",
    "outputId": "fdb9b708-93ad-4878-dbf6-38b9a414487b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ba414a37f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_index = 7777 \n",
    "print(y_train[image_index]) \n",
    "plt.imshow(x_train[image_index].reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFS5B2inIBDt"
   },
   "source": [
    "\n",
    "**Reshaping and Normalizing the Images**\n",
    "\n",
    "We all know that to use Keras API, we will need to have 4-dimension numpy arrays. But we see from the above code result we have 3-dimension numpy array. So, if you take a look at the code below, the first three lines explain how we can convert 3-dims to 4-dims. The next two lines are basically to have floating values after the division. Now coming to the normalzing part, we will always we to do this in our neural networks. This can be done by the last two lines of the code(before print statements) and all we need to do is we need to divide it by 255 (which is the maximum RGB code minus the minimum RGB code).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JhwpdE-8IH7I",
    "outputId": "1a384cba-3a1a-4809-a117-d7d87917067d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (63000, 28, 28, 1)\n",
      "Number of images in x_train 63000\n",
      "Number of images in x_test 7000\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFmUwBx_I7FE"
   },
   "source": [
    "\n",
    "**Building the Convolutional Neural Network**\n",
    "\n",
    "So we are creating our model by used Keras API and hence it will either have Tensorflow or Theano in backend. Therefore, I will import the Sequential Model from Keras and add Conv2D, MaxPooling, Flatten, Dropout, and Dense layers. To understand better, Dropout layers fight with the overfitting by disregarding some of the neurons while training while Flatten layers flatten 2D arrays to 1D array before building the fully connected layers. Please follow the below code on how to do it: ***You can play around with kernel size, pool size, activation functions, dropout rate, and number of neurons in the first Dense layer to get a better result.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Layers**\n",
    "\n",
    "The convolutional layer is the very first layer where we extract features from the images in our datasets. Due to the fact that pixels are only related to the adjacent and close pixels, convolution allows us to preserve the relationship between different parts of an image. Convolution is basically filtering the image with a smaller pixel filter to decrease the size of the image without losing the relationship between pixels. When we apply convolution to 5x5 image by using a 3x3 filter with 1x1 stride (1-pixel shift at each step). We will end up having a 3x3 output (64% decrease in complexity).\n",
    "\n",
    "**Pooling Layer**\n",
    "\n",
    "When constructing CNNs, it is common to insert pooling layers after each convolution layer to reduce the spatial size of the representation to reduce the parameter counts which reduces the computational complexity. In addition, pooling layers also helps with the overfitting problem. Basically we select a pooling size to reduce the amount of the parameters by selecting the maximum, average, or sum values inside these pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Q1G9fTL_iiyJ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(28, kernel_size=(2,2), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(84, kernel_size=(2,2), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "#model.add(Conv2D(112, kernel_size=(2,2), input_shape=input_shape))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "#Third was 4 * 28\n",
    "\n",
    "model.add(Flatten())\n",
    "#sigmoid\n",
    "#gelu\n",
    "#swish\n",
    "\n",
    "activation = tf.nn.swish\n",
    "#2 FC 128 Unit model #17\n",
    "#3 FC 64 128 256 model #18\n",
    "#model.add(Dense(64, activation=activation))\n",
    "model.add(Dense(128, activation=activation))\n",
    "#model.add(Dense(256, activation=activation))\n",
    "\n",
    "\n",
    "# 10 is the no of classes\n",
    "model.add(Dense(10,activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tESUNWpJUCi"
   },
   "source": [
    "\n",
    "**Compiling and Fitting the Model**\n",
    "\n",
    "We created an non-optimized empty CNN using the above code.Now we have to set an optimizer with a given loss function which uses a metric. Then, we can fit the model by using our train data. You can play around with the optimizer, loss function, metrics, and epochs.I can say that adam optimizer is usually out-performs the other optimizers, that's why I used that. You might think that Epoch number is a bit less, but in our cases it gives us a pretty amazing accuracy. Since the dataset we have does not require heavy computing power, you can play around with epoch numbers as well. Please follow the below code to have a better understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1969/1969 [==============================] - 31s 12ms/step - loss: 0.2786 - accuracy: 0.9182\n",
      "Epoch 2/10\n",
      "1969/1969 [==============================] - 23s 12ms/step - loss: 0.0449 - accuracy: 0.9864\n",
      "Epoch 3/10\n",
      "1969/1969 [==============================] - 24s 12ms/step - loss: 0.0263 - accuracy: 0.9917\n",
      "Epoch 4/10\n",
      "1969/1969 [==============================] - 25s 12ms/step - loss: 0.0167 - accuracy: 0.9946\n",
      "Epoch 5/10\n",
      "1969/1969 [==============================] - 26s 13ms/step - loss: 0.0138 - accuracy: 0.9953\n",
      "Epoch 6/10\n",
      "1969/1969 [==============================] - 26s 13ms/step - loss: 0.0109 - accuracy: 0.99621s - loss: 0.0108 - accuracy: 0.99 - ETA: 1s - loss: 0.0 - ETA: 1s - loss: 0.0109 - accuracy:  - ETA: 0s\n",
      "Epoch 7/10\n",
      "1969/1969 [==============================] - 25s 13ms/step - loss: 0.0111 - accuracy: 0.9963\n",
      "Epoch 8/10\n",
      "1969/1969 [==============================] - 24s 12ms/step - loss: 0.0081 - accuracy: 0.9972\n",
      "Epoch 9/10\n",
      "1969/1969 [==============================] - 26s 13ms/step - loss: 0.0083 - accuracy: 0.9971\n",
      "Epoch 10/10\n",
      "1969/1969 [==============================] - 36s 18ms/step - loss: 0.0082 - accuracy: 0.9972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ba483f42b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "#Adagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training. The more updates a parameter receives, the smaller the updates.\n",
    "#Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
    "#Adamax is sometimes superior to adam, specially in models with embeddings.\n",
    "#Gradient descent (with momentum) optimizer.\n",
    "\n",
    "#optimizer = keras.optimizers.Adagrad(learning_rate=0.001)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "#optimizer = keras.optimizers.Adamax(learning_rate=0.001)\n",
    "#optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(x=x_train,y=y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNfTf8uZMSil"
   },
   "source": [
    "\n",
    "**Evaluating the Model**\n",
    "\n",
    "Now we can go forward and evaluate the model by the below code. You will see that it has an accuracy of 98.5% and loss is very low. That means that using 10 epoch was a good fit for this use case. As this is a very basic model and our first model, we can consider that the model and the result is very good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aQ5fZekdK94e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0586 - accuracy: 0.9874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.058648522943258286, 0.9874285459518433]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThDrerMyMdeN"
   },
   "source": [
    "\n",
    "\n",
    "Now you can check if you model is trained well. Change the image_index and run the code to see if the number predicted is accurate or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 27, 27, 28)        140       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 28)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 84)        9492      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 84)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6, 6, 84)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               387200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 398,122\n",
      "Trainable params: 398,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#((Filter_size * 1) + 1) * filter_number\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GaFqiI3BMbPw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 2843\n",
    "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
    "predict = x_test[image_index].reshape(28,28)\n",
    "pred = model.predict(x_test[image_index].reshape(1, 28, 28, 1))\n",
    "print(pred.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.4",
   "language": "python",
   "name": "tf2.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
